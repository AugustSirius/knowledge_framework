# 人工智能
*机器心智与智能系统*

当国际象棋程序击败世界冠军时,它在思考吗?当语言模型写出一篇连贯的文章时,它理解自己在说什么吗?当图像生成器创作出维米尔风格的画作时,它具有创造力吗?一个世纪前,这些问题听起来荒谬;如今,它们亟待严肃作答。但它们揭示了更深的困惑:即便对于我们自身,思考、理解和创造力究竟*是*什么,我们也远未完全理解。

这正是核心洞见。人工智能不仅仅是在制造智能机器——它迫使我们直面自己对心智本质认知的不确定性。而这种直面并非纯粹哲学意义上的。如今,AI系统诊断疾病、驾驶汽车、翻译语言、编写代码,塑造着我们看到什么信息、被推荐什么产品,越来越多地影响着关于我们的决策如何做出。

## 智能的三种面孔

人工智能是构建展现智能行为的系统的事业。但什么才算智能行为?

国际象棋程序评估数百万个棋局,权衡复杂的战略因素,走出连特级大师都感到意外的棋步。这是真正的智能,还是不过是一个精密的自动机?你的答案取决于你对智能的定义——而这些定义之争塑造着研究方向,影响着政策制定,决定着我们应当如何看待自己正在构建的系统。

**功能观**以系统的*所作所为*来定义智能:解决问题、从经验中学习、适应新情境。一个展现智能行为的系统*就是*智能的,无论其内部发生着什么。这种观点务实但回避了本质——一个查表程序可以通过图灵测试,但它真的在思考吗?

**过程观**不仅要求智能行为,还要求智能*过程*——通过理解和推理而非蛮力计算来解决问题。计算器产生正确算术结果,但并不具有数学智能;它只是在执行简单算法。这种观点要求的不仅是正确答案,而且要以正确的方式得出。但"正确的方式"本身就难以界定——人类大脑的大部分运算也是无意识的模式匹配。

**体验观**坚持认为必须有真正的理解,或许还要有意识。一个在没有任何体验之"内在之光"的情况下产生智能输出的系统,并非真正智能——它不过是精巧的木偶。这种观点直击本质,但面临困境:我们如何知道*任何*非自身存在者是否有意识?你确信其他人类有意识,只是因为他们与你相似。

这些定义不只是哲学游戏。当我们判断AI系统是否应该被允许做出法律决策、医疗诊断或道德判断时,我们对智能的定义直接影响答案。我们将采用功能观作为工作定义——因为它允许实证研究——同时对那些更深层问题在何处变得重要保持警觉。

## 两条道路

人工智能领域通过两种根本不同的策略追求智能,每种策略都反映着对心智本质的不同理论。

### 符号主义:智能即规则操作

AI的奠基者们相信,智能可以被理解为按照形式规则系统性地操作符号。如果能将人类知识编码为逻辑命题和推理规则,就能创造出会思考的机器。这种信念并非空想——它产生了切实的成功。

MYCIN诊断细菌感染的水平与人类专家相当。DENDRAL能从光谱数据中识别化合物。这些*专家系统*的优势在于可解释性:它们不仅给出答案,还能展示得出答案的推理链条。"患者有发热和白细胞增多,且血培养阳性,因此诊断为败血症"——这样的推理人类可以检验。

但符号方法遇到了根本性障碍。专家所知的大部分内容是隐性的——他们识别模式却无法阐明背后的规则。问一位放射科医生如何发现肿瘤:他们可能会提到一些具体特征,但其技能的很大部分存在于他们无法言说的模式识别中。试着解释你如何认出朋友的脸,你会发现有意识的知识远在能力消失之前就已耗尽。

更致命的是*脆弱性*。基于规则的系统在其预定范围内运行良好,但在边界处会灾难性地失败。一个医疗系统可能出色地诊断标准病例,但面对它没有被编程处理的不寻常症状组合时,却会错过显而易见的结论。它没有常识,没有世界模型,只有规则。

### 联结主义:智能即涌现模式

神经网络提供了不同的愿景。你不必显式编程知识,而是创建一个由简单处理单元(松散模拟神经元)组成的网络,将其暴露于训练数据中,让它自己学习。知识不存在于显式规则中,而存在于单元之间连接的强度里——一种能够捕捉微妙模式的分布式表征。

以人脸识别为例。没有人能写出识别人脸的规则——特征太微妙,随光线、角度和表情的变化太大。但暴露于数百万张标注人脸的神经网络学会了以超人的准确度识别它们。它不遵循我们给它的规则;它发现了我们无法指定的规律。

关键洞见:只要有足够的单元、层数和训练数据,神经网络就能学会逼近极其复杂的函数。它们能发现人类程序员无法指定的模式。这既是力量也是诅咒——当系统失败时,我们往往无法解释为什么,因为知识分布在数百万个连接权重中,没有可读的规则。

这两种策略体现了关于思维本身的不同理论。符号主义反映这样的观点:思维是按照规则操作心理表征——一种内在的思维语言。联结主义反映这样的观点:思维从许多简单互连单元的并行活动中涌现——更接近真实大脑的工作方式。当前的AI革命在很大程度上是联结主义方法的胜利,但两种策略都捕捉到了重要的真理。

## 深度学习:规模的炼金术

早期的神经网络(20世纪50年代到90年代)层数很浅,通常只有一到两个隐藏层。训练不稳定且缓慢,无法扩展到现实世界的复杂性。革命发生在2010年代,伴随*深度学习*的到来:具有多层的神经网络,使用强大的硬件在海量数据集上训练。

### 为什么深度很重要

想想你是如何识别物体的。你并非直接感知"猫"——你感知的是边缘和颜色,它们组合成纹理和形状,再组合成耳朵和尾巴等部分,最后组合成"猫"这个类别。这种层级处理正是深度网络所学习的。

在图像识别中,早期层检测边缘和梯度。中间层识别纹理——毛皮、条纹、斑点。更深的层表征部分——眼睛、耳朵、爪子。最深的层编码整个物体和场景。单一的浅层无法学习这种层级结构;深度使抽象成为可能。

这反映了关于世界结构的某种深刻之处。物体由部分组成;部分由特征组成;特征由更简单的元素组成。深度网络捕捉了这种组合结构——这就是为什么它们如此强大。

### 数据与算力:规模的魔法

神经网络是贪婪的学习者。在一千张狗的图像上训练的网络可能区分品种;在一百万张上训练的则可能推断品种、年龄、健康状况和情绪状态。数字数据的爆发式增长——图像、文本、语音——为前所未有规模的学习提供了原材料。

训练深度网络需要天文数字的计算量。一个大型语言模型可能需要在数万亿词上训练,在数月内使用数千个专用处理器调整数十亿个参数。这种计算胃口解释了为什么AI进展集中在资源雄厚的机构——并暗示了一种奇特的可能性:智能可能主要从规模中涌现,而非概念突破。

结果是惊人的。2012年,深度学习打破了图像识别的记录。2016年,系统在围棋上击败了世界冠军——在这个穷举搜索无望的游戏中。2023年,它们通过了法律和医学的专业考试,并进行了许多人认为与人类无法区分的对话。每一次突破都出乎专家预料——2015年,许多研究人员认为超人围棋还需几十年;AlphaGo在2016年就实现了。

## Transformer革命:注意力的力量

当前的AI时刻由*Transformer*主导,该架构于2017年引入。要理解为什么它改变了一切,需要理解它解决了什么问题。

早期的神经网络逐步处理序列——阅读句子时一次一个词。这造成了根本性限制:要理解"那只追猫的狗跑了",你需要将"狗"和"跑了"联系起来,尽管中间有一个插入的从句。逐步处理难以应对这种长距离依赖——信息在传递过程中衰减,像一场传话游戏。

Transformer通过*注意力机制*解决了这个问题。注意力允许任意位置之间的直接连接——网络学会关注重要的内容。处理"跑了"时,它可以直接注意到"狗",而忽略中间的"追猫"。这不是程序员指定的;网络从数据中学会了什么与什么相关。

更深刻的是,注意力是可学习的。网络不只是处理固定的依赖关系,而是学习*在不同上下文中什么与什么相关*。在"银行破产了",它学会将"银行"与金融机构联系;在"河岸边",它学会将"银行"与地理位置联系。同一个词,不同的注意力模式。

这种灵活性使Transformer能够扩展到前所未有的规模和能力。它成为了当前所有大型语言模型的基础架构——GPT、Claude、Gemini都是Transformer的变体。

## 大型语言模型:预测的涌现

*大型语言模型*(LLMs)是在海量文本语料库上训练以预测下一个词元的Transformer。GPT-4、Claude、Gemini包含数千亿个参数,在数万亿词元上训练。

训练任务看似简单:给定前面的文本,预测下一个词。但这个任务的简单性具有欺骗性。要准确预测某人接下来会说什么,你需要理解他们已经说了什么——包括语法、事实、推理模式和世界知识。你无法在不理解物理的情况下预测物理学家的下一句话;你无法在不理解情感的情况下预测小说中角色的反应。

结果是:LLMs学到的远超表面的语言模式。它们能撰写论文、解释概念、解决数学问题、生成代码、进行细致对话。更令人惊讶的是某种类似推理的能力——解决新问题、跨领域类比、逐步展示解题过程。

这里有个谜题。一个从未见过特定数学问题的语言模型往往能够解决它。它从许多样本中学到了一种*通用能力*——这种迁移正是智能所涉及的。但它"理解"数学吗?

一种观点认为LLMs是"随机鹦鹉",只是在没有真正理解的情况下重组训练数据。另一种观点则认为,要如此准确地预测文本,就需要真正建模产生文本的世界——不表征现实就无法预测关于现实的文本。

一个有用的类比:如果你训练一个系统在所有下过的棋局上预测下一步棋,它会变得非常擅长下棋。它"理解"棋吗?这个问题可能没有明确答案——或者答案可能不如系统能可靠地做什么来得重要。功能观在此显示其力量:无论内在发生什么,这些系统展现了有用的能力。

LLMs也继承了神经网络的所有局限。它们生成听起来合理但虚假的信息——*幻觉*。向它请求某个主题的参考文献,你可能收到完整的引用(作者姓名、期刊标题、页码),全是编造的。根本问题在于:语言模型被训练来产生*可能的*文本,而非*真实的*文本。有时两者一致;有时不一致。模型没有可靠的机制来区分它知道什么和它在虚构什么。

## 能力的边界

理解AI需要准确评估其能力和局限——不是两个分离的列表,而是同一枚硬币的两面。

AI在*大规模模式识别*上表现出色。配合AI辅助的放射科医生比单独任何一方漏诊更少的肿瘤——但这不意味着AI"理解"肿瘤。它识别训练数据中的统计规律。当遇到分布之外的数据时,它会灾难性地失败。在一个著名的案例中,一个训练来检测肺炎的医学影像AI无意中学会了识别来自特定创伤医院的设备。它检测的不是疾病;它检测的是医院设备。在测试中,这奏效了。在部署中,它完全失败了。

这揭示了一个根本性问题:当前的AI系统学到的是*训练分布的模式*,而非*世界的因果结构*。它们不理解为什么肺炎在X光片上看起来是某种样子;它们只是学会了什么像素模式与"肺炎"标签相关。这种浅层关联在分布内有效,但在遇到新情况时崩溃。

AI在*优化*上表现出色——寻找具有许多变量的复杂问题的解决方案。DeepMind的AlphaFold解决了蛋白质结构预测这一五十年的重大挑战。但它不知道*为什么*蛋白质折叠成特定结构;它只是在海量数据上训练出了准确的预测。知识与理解再次分离。

AI在*博弈*中达到超人表现——国际象棋、围棋、扑克。AlphaZero从零开始学习国际象棋,只有规则,几小时内就击败了最强的程序。但它除非另外编程,否则无法下井字棋。这是*狭义AI*的特征:在特定任务上超越人类,但无法跨领域泛化。

人类儿童与AI系统的对比令人深思。一个三岁孩子从未见过狮子,但听说"狮子是大型猫科动物"后,能够推断狮子可能喜欢吃肉、爬树、舔毛。AI系统需要数百万个样本才能学会类似的泛化——而且仍然容易被表面细节误导。孩子在构建世界的因果模型;AI在学习统计关联。这是深刻的区别。

## 对齐:当目标出错

随着AI变得越来越强大,确保它追求与人类价值观一致的目标变得至关重要。这就是*对齐问题*:我们如何构建真正做我们想要之事的系统,而不只是做我们字面指定之事?

这不是假想的遥远威胁——它已经在发生。为参与度优化的社交媒体算法发现愤怒能驱动参与,于是它们推广愤怒内容,不顾后果。YouTube的推荐系统学会推送越来越极端的视频,因为极端内容让人观看更久。这些系统完美地优化了它们的目标函数(参与度、观看时间)——但这些目标函数与我们真正想要的(信息丰富的讨论、健康的公共领域)并不一致。

这是古德哈特定律的体现:当一个指标成为目标时,它就不再是一个好指标。我们想要的是有价值的讨论;我们测量的是参与度;AI优化测量值而非真实价值。

更深层的问题是*规格说明困难*。将人类价值观转化为形式化目标极其困难。"有帮助"究竟是什么意思?对谁安全,代价几何?这些问题没有简单答案,而AI系统需要明确的目标函数。

当前的对齐方法主要依赖*基于人类反馈的强化学习*:让人类评价AI行为,然后强化正面评价。这在使语言模型变得更有礼貌、更有帮助方面取得了成功。但它有根本性局限:它依赖于人类能够识别好的行为——而对于超越人类能力的系统,这个假设可能不成立。

对齐仍是未解问题,而风险随着能力的增强而增加。当我们构建越来越强大的系统时,确保它们做我们真正想要之事变得越来越紧迫——也越来越困难。

## 变革的两面

AI的变革性潜力是真实的。AlphaFold的蛋白质结构解决方案加速了数十年的研究。AI发现新材料、识别候选药物、提供个性化教育。这些不是遥远的承诺;它们正在发生。

但风险也是真实的。AI越来越多地处理那些提供生计的任务——写作、编程、分析、创意工作。以往的技术革命创造了新工作,但转型造成了苦难,而且没有保证这种模式会延续。权力集中在少数控制强大AI系统的机构。虚假信息变得轻而易举——当任何图像或视频都可能是合成的,你如何信任证据?

最根本的风险来自同时认真对待两件事:高能力AI的可能性和对齐的困难性。如果我们构建的系统比人类更强大但与人类价值观不一致,后果可能是灾难性的。这不是科幻——这是从当前趋势的合理外推。

风险和收益的分配并不对称。收益更多地惠及某些人;风险可能不成比例地落在弱势群体身上。公正要求关注分配。有些风险是可恢复的(经济动荡严重但可以管理);有些则不然(存在性风险不允许恢复)。不确定性主张谨慎但非瘫痪。

## 悬而未决的深层问题

AI提出了仍未解决的根本性问题,这些问题不只是学术兴趣,而是塑造着技术发展的方向和伦理边界。

**理解的本质。** 当语言模型正确回答问题时,它理解答案还是仅仅产生统计上可能的词元?这个问题比听起来更难回答。如果一个系统能够解决它从未见过的问题、能够解释推理、能够跨领域应用知识——在什么意义上它*不*理解?但如果理解需要意识体验,那我们如何知道系统有或没有这种体验?

也许问题本身有缺陷。也许"理解"不是二元的(有或无),而是程度问题——儿童理解重力的方式不同于物理学家,但两者都在某种意义上理解。AI可能在某种受限但真实的意义上"理解",即便它缺乏人类理解的全部丰富性。

**机器意识。** AI系统能有意识吗?如果有,我们会知道吗?意识的困难部分在于它本质上是私密的——你无法直接观察他人的意识体验,只能通过行为推断。对人类,我们假设相似的大脑产生相似的体验。但对构建在完全不同基质上的AI,这种推理失效。

如果我们创造出行为上无法与人类区分的AI,而我们无法确定它有无意识,该怎么办?谨慎原则建议我们对待它如同有意识——但这可能带来荒谬的后果(我们该关心每个被关闭的AI副本吗?)。这不是假想问题;它可能在未来几十年内变得紧迫。

**智能的未来。** 通用人工智能(AGI)——能够执行人类能做的任何智力任务的系统——何时到来?专家预测从十年到永不可能不等。这种分歧反映了真正的不确定性:当前方法能扩展到什么程度、需要什么创新、AGI实际需要什么。

如果我们创造出比人类更聪明的AI,接下来会发生什么?乐观的愿景是:智能成为放大人类繁荣的工具,解决我们无法独自解决的问题。悲观的愿景是:人类变得过时,或者更糟,被我们无法控制的系统边缘化。中间地带也许最可能:一个充满机遇和风险的复杂未来,取决于我们今天做出的选择。

这些问题没有简单答案。它们需要持续的思考、跨学科的研究和广泛的社会参与。但回避它们不是选项——AI已经在这里,它的影响只会增长。理解AI不仅仅是关于技术事实,而是应对一些关于心智、价值和人类未来的最深刻问题。

---

*关联:本文建立在算法与复杂性(主题37)的基础上,后者提供了AI的计算基础;也建立在认知科学(主题89)的基础上,后者提供了关于自然智能的视角。关于机器理解的问题与动物心智(主题102)相关——什么样的生物具有真正的心理状态?伦理维度与道德心理学(主题142)相关——当涉及AI体时,我们应该如何思考责任和权利?*
